import pyautogui
import time
import ollama
import pyperclip  # Module for accessing the clipboard

# Simulate initial click on Brave icon
pyautogui.click(1032, 1054)
time.sleep(2)

def is_last_msg_from_receiver(chat_history, user_name):
    """
    Check if the last message in the chat history is from someone other than the user.
    This function assumes the chat history contains messages with a pattern like:
    '[timestamp] Name: Message'.
    """
    lines = chat_history.split('\n')  # Split chat history into lines
    for line in reversed(lines):  # Process lines from the end
        if line.strip():  # Ignore empty lines
            if user_name not in line:  # If the user's name is not in the last message
                return True
            else:
                return False
    return False

def get_last_message(chat_history):
    """
    Extract the last non-empty message from the chat history.
    """
    lines = chat_history.split('\n')
    for line in reversed(lines):
        if line.strip():  # Ignore empty lines
            return line.strip()
    return ""

def automate_chat_response():
    model_name = "llama3.2:1b"
    user_name = "YourName"  # Replace this with your name or identifier in the chat
    last_processed_message = ""  # To track the last message already processed

    while True:
        # Pause for a moment to allow other actions to complete
        time.sleep(1)

        # Step 2: Drag from (714, 255) to (784, 932) to select text
        pyautogui.moveTo(714, 255)  # Move to starting point
        pyautogui.mouseDown()       # Press mouse down
        pyautogui.moveTo(784, 932, duration=1)  # Drag to endpoint (with 1-second duration)
        pyautogui.mouseUp()         # Release mouse button

        # Pause to ensure selection is made
        time.sleep(0.5)

        # Step 3: Copy the selected text to the clipboard
        pyautogui.hotkey('ctrl', 'c')  # Use Ctrl+C to copy
        time.sleep(0.5)  # Small delay to ensure clipboard is updated

        # Step 4: Get the copied text from the clipboard
        chat_history = pyperclip.paste()  # Retrieve the clipboard content
        print("Copied Text:", chat_history)

        # Extract the last message
        last_message = get_last_message(chat_history)

        # Check if the last message is new and NOT from the user
        if last_message != last_processed_message and is_last_msg_from_receiver(chat_history, user_name):
            last_processed_message = last_message  # Update the last processed message
            try:
                # Step 5: Create a request to the Ollama model
                response = ollama.chat(
                    model=model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": (
                                "You are Naruto, a friendly and helpful coder from India who speaks Hindi and English. "
                                "Respond to the user as Naruto based on the chat history provided."
                            )
                        },
                        {
                            "role": "user",
                            "content": chat_history
                        }
                    ]
                )
                
                # Verify if the response is generated
                if response and 'message' in response and 'content' in response['message']:
                    response_content = response['message']['content']
                    print("Generated Response:", response_content)
                    
                    # Step 6: Copy the response to the clipboard
                    pyperclip.copy(response_content)

                    # Step 7: Paste the response into the target field and press Enter
                    pyautogui.click(879, 947)  # Click on the input field
                    time.sleep(1)
                    pyautogui.hotkey('ctrl', 'v')  # Paste the response
                    time.sleep(1)
                    pyautogui.press('enter')  # Press Enter to send the response
                else:
                    print("No response generated by the Ollama model.")
            except Exception as e:
                print("Error while generating response:", str(e))

# Run the function
automate_chat_response()
